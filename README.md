
# Project 1: Relationsl Data Modeling (Postgres)
---

## Summary

A startup called Sparkify wants to analyze the data they've been collecting on songs and user activity on their new music streaming app, Their data resides in a directory of JSON logs on user activity on the app, as well as a directory with JSON metadata on the songs in their app.

This project is an implementation for ETL pipeline written in python for collecting and transforming Sparkify data from JSON logs and load them in a Postgres database to help Sparkify's analytics team to run customized queries for song play analysis to understand what songs users are listening to.

---


## Data Modeling

Using the star schema optimized for queries on song play analysis. This includes the following tables:

#### Fact Table

1.  **songplays**  - records in log data associated with song plays.

#### Dimension Tables

1.  **users**  - users in the app
2.  **songs**  - songs in music database
3.  **artists**  - artists in music database
4.  **time**  - timestamps of records in  **songplays**  broken down into specific units

---


## Data Sources

Data resides in two directories that contain files in JSON format:

1. **data/song_data** : Contains metadata about a song and the artist of that song;
2. **data/log_data** : Consists of log files generated by the streaming app based on the songs in the dataset above;

---


## How To Run

1. make sure your application can connect to local postgres server.
2. run `python create_tables.py` that will create the `sparkifydb` and tables.
3.  run `python etl.py` that will populate the database with the song play data from json logs so you can query the database.